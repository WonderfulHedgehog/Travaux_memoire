---
title: "Topic_Modeling_Tertullien"
author: "Mathilde"
date: "2023-07-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# I. Préparation des données

 1.1 Définition de la session de travail
# Indication du chemin vers le notebook
```{r}
setwd("~mathildeschwoerer/Documents/Humanités Numériques/Textes_memoire")
```

Le texte est déjà lemmatisé et structuré à l'intérieur d'un .csv. Pour retirer les stopwords, il sera transformé en chaîne de caractères. 

```{r}
df_Val <- read.csv("Lemma/val.tsv", sep="\t")
#Vérification que le formatage est correct
head(df_Val$lemma)
#Création d'une chaîne de caractère vide qui contiendra à l'avenir tous les textes contenus dans df. 
chaîne_de_caractères_Val <- ""
#Laisser le contenu vide : il n'y a rien pour l'instant.
```



 1.3.1 Importation de la liste de _stopwords_
 
```{r}
StopWords <- "Stops_latin.txt"
Stops = read.csv(StopWords, header=FALSE, stringsAsFactors=FALSE)[,]
```

On crée une chaîne de caractère ne contenant que les mots qui ne sont pas des stopwords et on convertit le tout en minuscules. 

```{r}
for (word in tolower(df_Val$lemma)) {
  if (!word %in% Stops) {
    chaîne_de_caractères_Val <- paste(chaîne_de_caractères_Val, word, sep=" ")
  }
}
chaîne_de_caractères_Val
chaîne_de_caractères_Val<-write(chaîne_de_caractères_Val ,file = "Val_1.txt")
```

 1.4. Création d'une liste pour une approche _bag_of_words_

On divise le texte en une dizaine de morceaux (10 est un chiffre arbitraire, on peut en mettre plus ou moins) et on met ces dix morceaux dans une liste qu'on peut par exemple baptiser Extraits. C'est ce qui permet l'approche _bag_of_words_.

Approche _bag_of_words_ : idée que le monde peut être décrit au moyen d'un dictionnaire. Dans sa version la plus simple, un document particulier est représenté par l'histogramme des occurrences des mots le composant : pour un document donné, chaque mot se voit affecté le nombre de fois qu'il apparaît dans le document (source : Wikipédia).

```{r}

```



```{r}
chaîne_de_caractères_corpus1 <-readLines("Clearer/Corpus_2.txt")
Nb_sequences <- 10
Extraits_corpus1<- strwrap(chaîne_de_caractères_corpus1, nchar(chaîne_de_caractères_corpus1) / Nb_sequences)
#On peut afficher le contenu de chaque séquence :
#Extraits_corpus1[1]
```


 1.5.Transformation en matrice vectorielle

Il faut installer les packages tm et tidytext.
```{r}
if(!require("tm")){
  install.packages("tm")
  library("tm")
}
if(!require("tidytext")){
  install.packages("tidytext")
  library("tidytext")
}

#Je transforme mes textes en corpus avec la fonction `corpus()`, un objet de classe `corpus` manipulable dans `R` contenant des données et des métadonnées.
#La fonction `VectorSource` transforme chaque document en vecteur.
corpus_corpus1 <- Corpus(VectorSource(Extraits_corpus1), readerControl = list(language = "lat"))
# J'affiche les informations à propos de ce corpus
corpus_corpus1
```


 1.6 Création d'un _document_term_matrix_

Un _document_term_matrix_ est une matrice mathématique qui décrit la fréquence des termes qui apparaissent dans une collection de documents.

```{r}
dtm_corpus1 <- DocumentTermMatrix(corpus_corpus1)
dtm_corpus1
```


#II. Analyse des données : fréquence des termes

 2.1.Graphe représentant la fréquence des termes
 Installation de la library pour le graphe et dessin du graphe

```{r}
freq_corpus1 <- as.data.frame(colSums(as.matrix(dtm_corpus1)))
colnames(freq_corpus1) <- c("frequence")
#as.data.frame est une fonction vérifiant qu'un objet est un dataframe ou le forçant à le devenir si c'est possible.
#colSums est une fonction permettant de former des sommes et des moyennes de lignes et de colonnes pour des tableaux et des dataframes.
#as.matrix est une fonction générique convertissant en matrice.
#colnames récupère ou définit le nom des lignes et des colonnes dans un objet de type matrice.
#c est une fonction générique qui combine ses arguments. La méthode par défaut combine les arguments pour former un vecteur.

#Pour dessiner un graphe, nécessité d'installer une nouvelle library: `ggplot2`
#gg = Grammar of Graphics
#Avec ggplot 2, les données représentées graphiquement proviennent toujours d'un dataframe.
if (!require("ggplot2")){
  install.packages("ggplot2")
  library("ggplot2")
}
ggplot(freq_corpus1, aes(x=frequence)) + geom_density()
```


 2.2 Analyse des données
 
 On retrouve la loi de Zipf dans la distribution des données.
 
 2.2.1 Mots avec de faibles fréquences
On peut compter les mots avec les fréquences faibles, par exemple avec moins de 100 occurrences (n+1).

```{r}
motsPeuFrequents_corpus1 <- findFreqTerms(dtm_corpus1, 0, 15)
length(motsPeuFrequents_corpus1)
head(motsPeuFrequents_corpus1,50)
```


 2.2.2 Mots avec de fortes fréquences
 On peut aussi compter et afficher les mots les plus fréquents, par exemple avec plus de 100 ou 200 occurrences.

```{r}
motsTresFrequents_corpus1 <- findFreqTerms(dtm_corpus1, 25, Inf)
length(motsTresFrequents_corpus1)
head(motsTresFrequents_corpus1,50)
```

```{r}
motsTresFrequents_corpus1 <- findFreqTerms(dtm_corpus1, 39, Inf)
length(motsTresFrequents_corpus1)
head(motsTresFrequents_corpus1,50)
```


 2.2.3.Association entre les mots

Daemonium
```{r}
findAssocs(dtm_corpus1, terms = "achamoth", corlimit = 0.85)
```

Lex
```{r}
findAssocs(dtm_Val, terms = "demiurgus", corlimit = 0.87)
```

 2.3 Nettoyage de la DTM pour éliminer les rangs vides.

```{r}
rowTotals <- apply(dtm_corpus1, 1, sum)      #On trouve la somme des mots dans chaque document.
dtm_corpus1_clean   <- dtm_corpus1[rowTotals> 0, ]    #On retire tous les documents sans mot.
```


#III. Topic Modeling

Un thème ( _topic_ ) est un _cluster_ de mots i.e. une récurrence de co-occurrences.

3.1 Installation de la library pour le _topic_modeling_

Comme le package "topicmodels" ne parvenait pas à s'installer, il a fallu télécharger la bibliothèque GSL (bibliothèque pour le calcul numérique en C et C++) via le terminal de l'ordinateur.

```{r}
if(!require("topicmodels")){
  install.packages("topicmodels")
  library("topicmodels")
}
```
 

 3.2 Les paramètres de Gibbs
 
 C'est une probabilité conditionnelle qui s'appuie, pour calculer le _β_ d'un mot, sur le _β_ des mots voisins. Pour ce faire, il faut déterminer:
1. À quel point un document aime un _topic_.
2. À quel point un _topic_ aime un mot.

 3.2.1 Installation de la library "ldatuning" pour déterminer le nombre optimal de topics
 
Pour installer cette library, il a été nécessaire de taper la commande suivante dans le terminal de l'ordinateur : sudo apt-get install libmpfr-dev (travail sous Linux).

```{r}
if(!require("ldatuning")){
  install.packages("ldatuning")
  library("ldatuning")
}
```

3.2.2 Détermination du nombre optimal de topics.
```{r}
#Exécution du calcul avec la fonction FindTopicsNumber
topicsNumber_corpus1 <- FindTopicsNumber(
  #La DTM utilisée est la suivante :
  dtm_corpus1_clean,
  #Le nombre de possibilités testées :
  topics = seq(from = 2, to = 20, by = 1),
  #Les métriques utilisées
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  verbose = TRUE #Si c'est FALSE, cela supprime tous les avertissments et les informations additionnelles.
)

#Utilisation de la fonction seq()qui permet de créer une séquence d'éléments dans un vecteur. La syntaxe est la suivante : seq (from, to, by, length.out) from = élément de début de la séquence ; to = élément de fin de la séquence ; by = différence entre les éléments ; length.out = longueur maximale du vecteur.

#Affichage du résultat
FindTopicsNumber_plot(topicsNumber_corpus1)
#Lecture du graph : "“Griffiths” et “Deveaud” suivent un principe de maximisation alors que “CaoJuan” et “Arun” obéissent à un principe de minimisation. Je vous épargne les détails techniques, mais l’idée ici est d’identifier l’endroit où simultanément “Griffiths” et “Deveaud” se rejoignent le plus et où c’est également le cas pour “CaoJuan” et “Arun”. Tout est histoire de compromis, trouver l’endroit ou l’écart entre les courbes est minimal en haut et en bas !" (source : https://ouvrir.passages.cnrs.fr/wp-content/uploads/2019/07/rapp_topicmodel.html)
```
 
 
 3.3.3 Exécution du calcul pour le topic modeling
```{r}
## Set parameters for Gibbs sampling
#Le modèle va tourner 2000 fois avant de commencer à enregistrer les résultats
burnin <- 2000
#Après cela il va encore tourner 2000 fois
iter <- 2000
# Il ne va enregistrer le résultat que toutes les 500 itérations
thin <- 500
#seed et nstart pour la reproductibilité
SEED=c(1, 2, 3, 4, 5)
seed <-SEED
nstart <- 5
#Seul le meilleur modèle est utilisé.
best <- TRUE

#4 topics
lda_gibbs_7_corpus1 <- LDA(dtm_corpus1_clean, 4, method="Gibbs", control=list(nstart=nstart, seed=seed, best=best, burnin=burnin, iter=iter, thin=thin))
#Utilisation de la fonction LDA avec la dtm utilisée, le nombre de topics, la méthode et le contrôle appliqué.
```

On peut désormais voir les premiers résultats. Il s'agit des mots dont la fréquence d'utilisation est corrélée.

```{r}
"LDA GIBBS 4"
termsTopic_lda_gibbs_7_corpus1 <- as.data.frame(terms(lda_gibbs_7_corpus1,10))
head(termsTopic_lda_gibbs_7_corpus1,11)
```

Nous allons utiliser `lda_gibbs_4_PH_V` et construire une matrice avec les _β_ des tokens (pour les ɣ, et donc des probabilités par document, on aurait mis `matrix = "gamma"`). Chaque token est répété deux fois, avec une probabilité pour chaque _topic_:

```{r}
topics_corpus1 <- tidy(lda_gibbs_7_corpus1, matrix = "beta")
topics_corpus1
```

#IV. Visualisation

 4.1 Récupération des mots

 4.1.1 Installation de la library "dplyr"
Cette library facilite le traitement et la manipulation de données contenues dans une ou plusieurs tables en proposant une syntaxe sous forme de verbes.
```{r}
if (!require("dplyr")){
   install.packages("dplyr")
  library("dplyr")
}
```

 4.1.2 Affichage des mots récupérés dans un graphe

```{r}
#Recupération des mots
top_terms_corpus1 <- topics_corpus1 %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup()  %>%
  arrange(topic, -beta)
#Dessin du graphe
#On retrouve la fonction ggplot, cette fois-ci avec geom_col qui permet de créer des diagrammes à barres (barplots).
top_terms_corpus1 %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) + geom_col(show.legend = FALSE) +
                                                  facet_wrap(~ topic, scales = "free") +
                                                  coord_flip() +
                                                  scale_x_reordered()
```
```{r}
# Load the required packages
library(topicmodels)
library(tidytext)
library(ggplot2)

# Convert the topic model to a tidy format
topics_tidy <- tidy(lda_gibbs_7_corpus1, matrix = "beta")

# Calculate the Jensen-Shannon divergence between topics
topic_proportions <- topics_tidy %>%
  group_by(topic) %>%
  summarize(total = sum(beta))

topic_distances <- topics_tidy %>%
  left_join(topic_proportions, by = "topic") %>%
  mutate(beta_normalized = beta / total) %>%
  pivot_wider(names_from = topic, values_from = beta_normalized) %>%
  select(-total) %>%
  select(-term) %>%
  column_to_rownames(var = "topic") %>%
  topicmodels:::.JSD()

# Convert the distances to a data frame for plotting
distance_df <- as.data.frame(as.matrix(topic_distances))
distance_df <- melt(distance_df)

# Create a scatter plot of topic distances
ggplot(distance_df, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "blue") +
  labs(x = "Topic", y = "Topic", title = "Topic Distance Heatmap")
```

